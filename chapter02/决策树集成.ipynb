{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树集成\n",
    "**集成（ensemble）**是合并多个机器学习模型来构建更强大模型的方法。在机器学习文献中有许多模型都属于这一类，但已注明有两种集成模型对大量分类和回归的数据集都是有效的，二者都已决策树为基础，分别是**随机森林（random forest）**和**梯度提升决策树（gradient boosted decision tree）**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]\n",
      "pandas version:0.23.4\n",
      "matplotlib version:3.0.2\n",
      "Numpy version:1.15.4\n",
      "Scipy version:1.1.0\n",
      "IPython version:7.2.0\n",
      "scikit-learn version:0.20.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version:{}\".format(sys.version))\n",
    "\n",
    "import pandas as pd\n",
    "print(\"pandas version:{}\".format(pd.__version__))\n",
    "\n",
    "import matplotlib\n",
    "print(\"matplotlib version:{}\".format(matplotlib.__version__))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "print(\"Numpy version:{}\".format(np.__version__))\n",
    "\n",
    "import scipy as sp\n",
    "print(\"Scipy version:{}\".format(sp.__version__))\n",
    "\n",
    "import IPython\n",
    "print(\"IPython version:{}\".format(IPython.__version__))\n",
    "\n",
    "import sklearn\n",
    "print(\"scikit-learn version:{}\".format(sklearn.__version__))\n",
    "\n",
    "import mglearn\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机森林\n",
    "随机森林是为了解决决策树经常过拟合的一种方法。随机森林本质上是许多决策树的集合，其中每棵树和其他树略有不同。随机森林背后的思想是，每棵树的预测可能都相对较好，但可能对部分数据过拟合。如果构造很多树，并且每棵树的预测都很好，但都以不同的方式过拟合，那么我们可以对这些树的结果取平均值来降低过拟合。既能减少过拟合又能保持树的预测能力，这可以在数学上严格证明。  \n",
    "为了实现这一策略，我们需要构造许多决策树，每棵树都应该对目标值做出可以接受的预测，还应该与其他树不同。随机森林的名字来自于将随机性添加到树的构造过程中，以确保每棵树都各不相同。随机森林中树的随机化方法有两种：一种是通过选择用于构造树的数据点，另一种是通过选择每次划分测试的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造随机森林\n",
    "想要构造一个随机森林模型，你需要确定用于构造的树的个数（RandomForestRegressor或RandomForestClassifier的n_estimators参数）。比如我们需要构造10棵树。这些树在构造时彼此完全独立，算法对每棵树进行不同的随机选择，以确保树和树之间是有区别的。想要构造一棵树，首先要对数据进行自助采样（boostrap sample）。也就是说，从n_samples个数据点钟有放回地（即同一样本可以被多次抽取）重复随机抽取一个样本，共抽取n_samples次。这样会创建一个与原数据集大小相同的数据集，但有些数据点会缺失（大约三分之一），有些会重复。  \n",
    "接下来，基于这个新创建的数据集来构造决策树。但是这里要对数据集的特征选取稍作修改：在每个结点处，算法随机选择特征的一个子集，并对其中一个特征寻找最佳测试，而不是对每个结点都寻找最佳测试。选择的特征个数由max_features参数来控制。每个结点中特征子集的选择是相互独立的，这样树的每个结点可以使用特征的不同子集来做出决策。  \n",
    "由于使用了自助采样，随机森林中构造每棵决策树的数据集都略有不同的。由于每个结点的特征选择，每棵树中的每次划分都是基于特征的不同子集。这两种方法共同保证随机森林总所有树都不相同。  \n",
    "在这个过程中的一个关键参数是max_features。如果我们设置max_features等于n_features,那么每次划分都要考虑数据集的所有特征，在特征选择的过程中没有添加随机性（不过自助采样依然存在随机性）。如果设置max_featues等于1，那么在划分时将无法选择对哪个特征进行测试，只能对随机选择的某个特征搜索不同的阈值。因此，如果max_features较大，那么随机森林中的树将会十分相似，利用最独特的特征可以轻松拟合数据。如果max_features较小，那么随机森林中的树将会差异很大，为了很好地拟合数据，每棵树的深度都要很大。  \n",
    "想要利用随机森林较小预测，算法首先对森林中的每棵树进行预测。对于回归问题，我们可以对这些结果取平均值作为最终预测。对于分类问题，则用到了“软投票”（soft voting）策略。也就是说，每个算法做出“软”预测，给出每个可能的输出标签的概率。对所有树的预测概率取平均值，然后将概率最大的类别作为预测结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
